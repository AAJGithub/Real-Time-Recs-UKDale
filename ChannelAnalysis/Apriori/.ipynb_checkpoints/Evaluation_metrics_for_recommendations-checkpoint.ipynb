{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8xEFYl3SYfo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels_file_path):\n",
    "    \"\"\"\n",
    "    This method gets channel labels from the labels.dat file from the house directory\n",
    "    \n",
    "    Input:\n",
    "    labels_file_path = Path to the labels.dat file.\n",
    "    \n",
    "    Output:\n",
    "    labels_df = Channel and appliance name dataframe.\n",
    "    labels_dict = Channel and appliance name dictionary.\n",
    "    \"\"\"\n",
    "    labels_df = pd.read_csv(labels_file_path, sep='\\\\s+', names=['Channel_id', 'Appliance'])\n",
    "    labels_df[\"Channel_id\"] = [\"channel_\"+str(i) for i in range(1,labels_df.shape[0]+1)]\n",
    "    labels_dict = dict()\n",
    "    for row in labels_df.iterrows():\n",
    "        labels_dict[row[1][\"Channel_id\"]] = row[1][\"Appliance\"]\n",
    "    return labels_df, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_list(house_path):\n",
    "    \"\"\"\n",
    "    This method gets the list of channels from the house directory.\n",
    "    \n",
    "    Input:\n",
    "    house_path = path to the house directory.\n",
    "    \n",
    "    Output:\n",
    "    channel_list = list of all the channels except channel_1 (mains).\n",
    "    \"\"\"\n",
    "    if(house_path[-1] != '/'):\n",
    "        house_path = house_path + '/'\n",
    "    channel_list = []\n",
    "    for item in os.listdir(path):\n",
    "        if 'channel_' in item and item != \"channel_1.dat\":\n",
    "            channel_list.append(item[:-4])\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJOlnYKdUlio"
   },
   "outputs": [],
   "source": [
    "def get_channel_data(on_off_data_dir_path, channel_list):\n",
    "    \"\"\"\n",
    "    This method loads channel on/off data from .npy files \n",
    "    generated by running 'Resampling_and_generating_appliance_on_off_data' file \n",
    "    \n",
    "    Input:\n",
    "    file_path = Path to the .npy files for all the channels\n",
    "    equipments = List of channels for the house being analyzed\n",
    "    \n",
    "    Output:\n",
    "    equip_dict = Dictionary of equipment and the datetime data when the equipment was turned on\n",
    "    \"\"\"\n",
    "    channel_dict = dict()\n",
    "    for channel in channel_list:\n",
    "        if 'channel_' in channel and channel != \"channel_1.dat\":    \n",
    "            # print( on_off_data_dir_path + channel)\n",
    "            channel_data = np.load(on_off_data_dir_path + channel + \".npy\")\n",
    "            channel_data = list(channel_data)\n",
    "            # Filter out only those instances where appliance is on\n",
    "            channel_data = [ x for x in channel_data if x!= '0']\n",
    "            # Populating dictionary with channel data using channel name as key\n",
    "            channel_dict[channel] = list(channel_data) \n",
    "    return channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_dates(path_to_resampled_channel_data, channel_list):\n",
    "    \"\"\"\n",
    "    This method finds the range of dates in which all the appliances were recorded //\n",
    "    and returns a min and a max date for given appliances. \n",
    "    \n",
    "    Input:\n",
    "    path_to_resampled_channel_data = Path to the resampled data directory\n",
    "    number_of_channels = Total number of channels in the house\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    min_date = The earliest date on which an appliance usage was recorded.\n",
    "    max_date = The latest date on which an appliance usage was recorded.\n",
    "    \"\"\"\n",
    "    min_date = datetime.datetime.max.date()\n",
    "    max_date = datetime.datetime.min.date()\n",
    "\n",
    "    for i in reversed(range(2, len(channel_list) + 1)):\n",
    "        cd = np.load(path_to_resampled_channel_data + \"channel_\"+str(i)+\".npy\")\n",
    "        for item in cd:\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date < min_date):\n",
    "                    min_date = temp_date\n",
    "                break;\n",
    "\n",
    "        for item in reversed(cd):\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date > max_date):\n",
    "                    max_date = temp_date\n",
    "                break;\n",
    "    return min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_list(min_date, max_date):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of date strings in between these dates\n",
    "    \"\"\"\n",
    "    Dates = []\n",
    "    start_dt = min_date\n",
    "    end_dt = max_date\n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        Dates.append(dt.strftime(\"%Y-%m-%d\"))\n",
    "    return Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of dates in between these dates\n",
    "    \"\"\"\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kM4-_W0udTzB"
   },
   "outputs": [],
   "source": [
    "def get_all_times_of_day(interval):\n",
    "    \"\"\"\n",
    "    This method generates a list of times of a day seperated by specified interval.\n",
    "    \n",
    "    Input:\n",
    "    interval = The gap between two neighboring time slots\n",
    "    \n",
    "    Output:\n",
    "    Time = List of times seperated by specified interval\n",
    "    \"\"\"\n",
    "    hour = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "    minute = ['00', str(interval)]\n",
    "    second = '00'\n",
    "    Time = []\n",
    "    for hr in hour:\n",
    "        for min in minute:\n",
    "            temp = [hr, min, second]\n",
    "            temp = ':'.join(temp)\n",
    "            Time.append(temp)\n",
    "    return Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcIT8idqnM4O"
   },
   "outputs": [],
   "source": [
    "def get_usage_data_for_day(channel_data_dict, channel_list, date):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular day. \n",
    "    \n",
    "    Input:\n",
    "    channel_data_dict = Dictionary of channel and their on/off data.\n",
    "    channel_list = List of channels for this house\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    \n",
    "    Output:\n",
    "    cleaned_day_data = Dictionary of appliances used that day and their usage (on timings).\n",
    "    \n",
    "    \"\"\"\n",
    "    day_data = dict()\n",
    "    for channel in channel_list:\n",
    "        channel_data = list(channel_data_dict[channel])\n",
    "        channel_data = [ x for x in channel_data if date in x]\n",
    "        day_data[channel] = channel_data\n",
    "    cleaned_day_data = dict()    \n",
    "    for channel in day_data.keys():\n",
    "        if len(day_data[channel]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_day_data[channel] = day_data[channel]\n",
    "    no_of_channel_w = len(cleaned_day_data.keys()) \n",
    "#     print(\"No of channel working on {%s} are :\"%(date),end = \" \")\n",
    "#     print(no_of_channel_w)       \n",
    "    return cleaned_day_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unVB8HYd6-uJ"
   },
   "outputs": [],
   "source": [
    "def get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular time of the day. \n",
    "    \n",
    "    Input:\n",
    "    day_data = Dictionary of channel and their on/off data for the day.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    time = Time for which used appliances and their usage needs to be extracted\n",
    "    channel_list = List of channels for this house\n",
    "        \n",
    "    Output:\n",
    "    list = List of channels on for that time of the day.\n",
    "    \n",
    "    \"\"\"\n",
    "    temp_list = [date, time]\n",
    "    temp_time = ' '.join(temp_list)\n",
    "    time_data = dict()\n",
    "    time_data[time] = []\n",
    "    for channel in day_data.keys():\n",
    "        temp_data = list(channel_data_dict[channel])\n",
    "        temp_data = [ x for x in temp_data if temp_time in x]\n",
    "        if len(temp_data)==0:\n",
    "            continue\n",
    "        else:\n",
    "            time_data[channel] = temp_data\n",
    "#     print(\"No of channels working at %s %s are :\"%(date, time),end =\" \")\n",
    "#     print(len(time_data.keys()))       \n",
    "    return list(time_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiUiRUO4e2qE"
   },
   "outputs": [],
   "source": [
    "def data_extractor(channel_list, channel_data_dict, dates, times):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a given date range. \n",
    "    \n",
    "    Input:\n",
    "    channel_list = List of channels for this house.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    dates = Dates for which used appliances and their usage needs to be extracted.\n",
    "    times = Times for which used appliances and their usage needs to be extracted.\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    list = List-of-lists of channels that are On for that period.\n",
    "    \n",
    "    \"\"\"\n",
    "    transactions = []\n",
    "    no_of_ch = len(channel_list)\n",
    "    # Iterate over dates in the date range\n",
    "    for date in dates:\n",
    "        # Get appliance usage data for day\n",
    "        day_data = get_usage_data_for_day(channel_data_dict, channel_list, date)\n",
    "        # Iterate over times in the 24 hour time range\n",
    "        for time in times:\n",
    "            # Get appliances on for time\n",
    "            temp_list = get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list)           \n",
    "            if len(temp_list)==0:\n",
    "                continue\n",
    "            else:\n",
    "                transactions.append(temp_list)\n",
    "    return transactions                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data_into_time(times, apriori_data):\n",
    "    \"\"\"\n",
    "    This method seperates ON appliance sequences by time and Saves them in a dictionary with time slices as the key\n",
    "    \n",
    "    Input:\n",
    "    times = List of time slices.\n",
    "    apriori_data = Output of the data_extractor().\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets.\n",
    "    \"\"\"\n",
    "    time_sequence_dict = dict()\n",
    "    for timestamp in times:\n",
    "        sequence_list = []\n",
    "        for data in apriori_data:\n",
    "            if timestamp in data:\n",
    "                sequence_list.append(data)\n",
    "        time_sequence_dict[timestamp] = sequence_list\n",
    "    return time_sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_and_itemsets(apriori_data, minimum_support):\n",
    "    \"\"\"\n",
    "    This method runs the apriori algorithm on the input data and returns frequent itemsets with their respective supports\n",
    "    \n",
    "    Input:\n",
    "    apriori_data = List-of-lists of appliances On at a particular time for each day.\n",
    "    minimum_support = Minimum support for getting the frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    data = te.fit(apriori_data).transform(apriori_data)\n",
    "    data = pd.DataFrame(data, columns = te.columns_)\n",
    "    return apriori(data, min_support = minimum_support, use_colnames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_frequent_itemsets(frequent_itemsets_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the support-frequent itemsets dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    frequent_itemsets_df = Dataframe of support and frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the entire frequent itemsets\n",
    "    \"\"\"\n",
    "    channels = set()\n",
    "    for item in frequent_itemsets_df.itemsets:\n",
    "        for entry in list(item):\n",
    "            channels.add(entry)\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appliances_from_channels(list_of_channels, labels_df):\n",
    "    \"\"\"\n",
    "    This method returns a list of appliance names given a list of channels.\n",
    "    \n",
    "    Input:\n",
    "    list_of_channels = List of channels for which names to be extracted.\n",
    "    labels_df = Dataframe of channel ids and appliance names.\n",
    "    \n",
    "    Output:\n",
    "    List of appliance names corresponding to each channel in list_of_channels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in list_of_channels:\n",
    "        result.append(labels_df.set_index('Channel_id').at[item, 'Appliance'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_rules(rule_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_list = []\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = str(sequence)[12:-3].replace(\"'\",'').split(\", \")\n",
    "        for item in list_of_channels:\n",
    "            if item not in channel_list:\n",
    "                channel_list.append(item)\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_channel_list(rule_df, full_channel_set):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels not present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    full_channel_set = Set of complete channels in the house\n",
    "    \n",
    "    Output:\n",
    "    Set of channels not present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_set = set()\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = sequence[12:-3].replace(\"'\",'').split(\", \")\n",
    "        channel_set.update(list_of_channels)\n",
    "    result_1 = full_channel_set - channel_set\n",
    "    return result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(recs, show_name_dict):\n",
    "    res = []\n",
    "    for item in recs:\n",
    "        res.append(show_name_dict[item])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_janhavis_recs_to_csv(recs_df):\n",
    "    recommendation_list = []\n",
    "    for item in recs_df['Recommendations']:\n",
    "        recs = []\n",
    "        item = item[1:-1]\n",
    "        for channel in item.split(\", \"):\n",
    "            channel = channel[2:-2]\n",
    "            recs.append(channel)\n",
    "        recommendations = get_appliances_from_channels(recs, labels_df)\n",
    "        recommendations = change_names(recommendations, show_name_dict)\n",
    "        recommendation_list.append(\",\".join(appliance for appliance in recommendations))\n",
    "    time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})\n",
    "    return time_recommendation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(Dates, Time, channel_data):\n",
    "    datetime_channel_dict = dict()\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            datetime_channel_dict[date + \" \" + timestamp] = set()\n",
    "            for channel in channel_data.keys():\n",
    "                if(date + \" \" + timestamp in channel_data[channel]):\n",
    "                    datetime_channel_dict[date + \" \" + timestamp].add(channel)\n",
    "    return datetime_channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_recommendations(k, Dates, recs_file_path):\n",
    "    datetime_recs_dict = dict()\n",
    "    recs_df = pd.read_csv(recs_file_path, header=0, names=['Time','Recommendations'])\n",
    "    for date in Dates:\n",
    "        for index,row in recs_df.iterrows():\n",
    "            time = row['Time']\n",
    "            recommendations = row['Recommendations']    \n",
    "            datetime_recs_dict[date + \" \" + time] = set(list(recommendations.split(\",\")[:k]))\n",
    "    return datetime_recs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(Dates, Time, ground_truth, recommendations):\n",
    "    count = 0\n",
    "    precision = 0\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            relevant = ground_truth[date + \" \" + timestamp]\n",
    "            recommended = recommendations[date + \" \" + timestamp]\n",
    "            if(len(relevant) == 0):\n",
    "                continue\n",
    "            count += 1\n",
    "            temp_precision = (len(relevant.intersection(recommended))/len(recommended))\n",
    "            precision += temp_precision\n",
    "    avg_precision = precision/count\n",
    "    return avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(Dates, Time, ground_truth, recommendations):\n",
    "    count = 0\n",
    "    recall = 0\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            relevant = ground_truth[date + \" \" + timestamp]\n",
    "            recommended = recommendations[date + \" \" + timestamp]\n",
    "            if(len(relevant) == 0):\n",
    "                continue\n",
    "            count += 1\n",
    "            temp_recall = (len(relevant.intersection(recommended))/len(relevant))\n",
    "            recall += temp_recall\n",
    "    avg_recall = recall/count\n",
    "    return avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaggregated data\n",
    "\n",
    "# path = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/\"\n",
    "# resampling_time_in_min = '30'\n",
    "# min_confidence = 0.4\n",
    "# min_support = 0.1\n",
    "# considered_rules = 200\n",
    "# labels_file = \"../../Disaggregation/House_2/labels.dat\"\n",
    "# apriori_data_output_file = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/apriori_data.npy\"\n",
    "# rule_files_dir = \"./Disaggregated_Rule_Files/\"\n",
    "# recommendations_file = './disaggregated_recommendations_01_04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "house = 'House_2'\n",
    "min_support = 0.02\n",
    "min_confidence = 0.02\n",
    "considered_rules = 200\n",
    "resampling_time_in_min = '30'\n",
    "path = \"../Utils/Channel_On_Off_data/\" + house + \"/\"\n",
    "labels_file = \"../../../../Dataset/ukdale/\" + house + \"/labels.dat\"\n",
    "rule_files_dir = \"./Train/Rule_Files/\" + house + \"/\"\n",
    "recommendations_file = \"./Train/Recommendations/\" + house + \"/recommendations_\" + str(min_support) + \"_\" + str(min_confidence) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/\"\n",
    "# resampling_time_in_min = '30'\n",
    "# min_confidence = 0.4\n",
    "# min_support = 0.1\n",
    "# considered_rules = 200\n",
    "# labels_file = \"../../Disaggregation/House_2/labels.dat\"\n",
    "# apriori_data_output_file = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/apriori_data.npy\"\n",
    "# rule_files_dir = \"./Disaggregated_Rule_Files/\"\n",
    "# recommendations_file = './disaggregated_recommendations_01_04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 2 Lables\n",
    "\n",
    "show_name_dict = dict()\n",
    "show_name_dict['laptop'] = \"Laptop\"\n",
    "show_name_dict['monitor'] = \"Monitor\"\n",
    "show_name_dict['speakers'] = \"Speakers\"\n",
    "show_name_dict['server'] = \"Server\"\n",
    "show_name_dict['router'] = \"Router\"\n",
    "show_name_dict['server_hdd'] = \"Server_hdd\"\n",
    "show_name_dict['kettle'] = \"Kettle\"\n",
    "show_name_dict['rice_cooker'] = \"Rice Cooker\"\n",
    "show_name_dict['running_machine'] = \"Running Machine\"\n",
    "show_name_dict['laptop2'] = \"Laptop2\"\n",
    "show_name_dict['washing_machine'] = \"Washing Machine\"\n",
    "show_name_dict['dish_washer'] = \"Dish Washer\"\n",
    "show_name_dict['fridge'] = \"Fridge\"\n",
    "show_name_dict['microwave'] = \"Microwave\"\n",
    "show_name_dict['toaster'] = \"Toaster\"\n",
    "show_name_dict['playstation'] = \"Playstation\"\n",
    "show_name_dict['modem'] = \"Modem\"\n",
    "show_name_dict['cooker'] = \"Cooker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 3 Lables\n",
    "\n",
    "# show_name_dict = dict()\n",
    "# show_name_dict['laptop'] = \"Laptop\"\n",
    "# show_name_dict['kettle'] = \"Kettle\"\n",
    "# show_name_dict['electric_heater'] = \"Electric Heater\"\n",
    "# show_name_dict['projector'] = \"Projector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 4 Lables\n",
    "\n",
    "# show_name_dict = dict()\n",
    "# show_name_dict['tv_dvd_digibox_lamp'] = \"TV & DVD Digibox & Lamp\"\n",
    "# show_name_dict['kettle_radio'] = \"Kettle & Radio\"\n",
    "# show_name_dict['gas_boiler'] = \"Gas Boiler\"\n",
    "# show_name_dict['freezer'] = \"Freezer\"\n",
    "# show_name_dict['washing_machine_microwave_breadmaker'] = \"Washing Machine & Microwave & Breadmaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels from the labels.dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df, labels_map = get_labels(labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_id</th>\n",
       "      <th>Appliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>channel_1</td>\n",
       "      <td>aggregate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>channel_2</td>\n",
       "      <td>laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>channel_3</td>\n",
       "      <td>monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>channel_4</td>\n",
       "      <td>speakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>channel_5</td>\n",
       "      <td>server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>channel_6</td>\n",
       "      <td>router</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>channel_7</td>\n",
       "      <td>server_hdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>channel_8</td>\n",
       "      <td>kettle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>channel_9</td>\n",
       "      <td>rice_cooker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>channel_10</td>\n",
       "      <td>running_machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>channel_11</td>\n",
       "      <td>laptop2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>channel_12</td>\n",
       "      <td>washing_machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>channel_13</td>\n",
       "      <td>dish_washer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>channel_14</td>\n",
       "      <td>fridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>channel_15</td>\n",
       "      <td>microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>channel_16</td>\n",
       "      <td>toaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>channel_17</td>\n",
       "      <td>playstation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>channel_18</td>\n",
       "      <td>modem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>channel_19</td>\n",
       "      <td>cooker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Channel_id        Appliance\n",
       "0    channel_1        aggregate\n",
       "1    channel_2           laptop\n",
       "2    channel_3          monitor\n",
       "3    channel_4         speakers\n",
       "4    channel_5           server\n",
       "5    channel_6           router\n",
       "6    channel_7       server_hdd\n",
       "7    channel_8           kettle\n",
       "8    channel_9      rice_cooker\n",
       "9   channel_10  running_machine\n",
       "10  channel_11          laptop2\n",
       "11  channel_12  washing_machine\n",
       "12  channel_13      dish_washer\n",
       "13  channel_14           fridge\n",
       "14  channel_15        microwave\n",
       "15  channel_16          toaster\n",
       "16  channel_17      playstation\n",
       "17  channel_18            modem\n",
       "18  channel_19           cooker"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get house related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel list and data from channel files\n",
    "channel_list = get_channel_list(path)\n",
    "channel_data = get_channel_data(path, channel_list)\n",
    "\n",
    "# Get max and min date in the channel data\n",
    "min_date, max_date = get_min_max_dates(path, channel_list)\n",
    "\n",
    "# Generate date and time list \n",
    "Dates = get_dates_list(min_date, max_date)\n",
    "\n",
    "train_dates = Dates[:round(len(Dates)*0.9)]\n",
    "test_dates = Dates[round(len(Dates)*0.9):]\n",
    "\n",
    "Time = get_all_times_of_day(resampling_time_in_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate rules for each time of the day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori input data generated!\n",
      "Generating Itemsets for : 00:00:00\n",
      "Generating Itemsets for : 00:30:00\n",
      "Generating Itemsets for : 01:00:00\n",
      "Generating Itemsets for : 01:30:00\n",
      "Generating Itemsets for : 02:00:00\n",
      "Generating Itemsets for : 02:30:00\n",
      "Generating Itemsets for : 03:00:00\n",
      "Generating Itemsets for : 03:30:00\n",
      "Generating Itemsets for : 04:00:00\n",
      "Generating Itemsets for : 04:30:00\n",
      "Generating Itemsets for : 05:00:00\n",
      "Generating Itemsets for : 05:30:00\n",
      "Generating Itemsets for : 06:00:00\n",
      "Generating Itemsets for : 06:30:00\n",
      "Generating Itemsets for : 07:00:00\n",
      "Generating Itemsets for : 07:30:00\n",
      "Generating Itemsets for : 08:00:00\n",
      "Generating Itemsets for : 08:30:00\n",
      "Generating Itemsets for : 09:00:00\n",
      "Generating Itemsets for : 09:30:00\n",
      "Generating Itemsets for : 10:00:00\n",
      "Generating Itemsets for : 10:30:00\n",
      "Generating Itemsets for : 11:00:00\n",
      "Generating Itemsets for : 11:30:00\n",
      "Generating Itemsets for : 12:00:00\n",
      "Generating Itemsets for : 12:30:00\n",
      "Generating Itemsets for : 13:00:00\n",
      "Generating Itemsets for : 13:30:00\n",
      "Generating Itemsets for : 14:00:00\n",
      "Generating Itemsets for : 14:30:00\n",
      "Generating Itemsets for : 15:00:00\n",
      "Generating Itemsets for : 15:30:00\n",
      "Generating Itemsets for : 16:00:00\n",
      "Generating Itemsets for : 16:30:00\n",
      "Generating Itemsets for : 17:00:00\n",
      "Generating Itemsets for : 17:30:00\n",
      "Generating Itemsets for : 18:00:00\n",
      "Generating Itemsets for : 18:30:00\n",
      "Generating Itemsets for : 19:00:00\n",
      "Generating Itemsets for : 19:30:00\n",
      "Generating Itemsets for : 20:00:00\n",
      "Generating Itemsets for : 20:30:00\n",
      "Generating Itemsets for : 21:00:00\n",
      "Generating Itemsets for : 21:30:00\n",
      "Generating Itemsets for : 22:00:00\n",
      "Generating Itemsets for : 22:30:00\n",
      "Generating Itemsets for : 23:00:00\n",
      "Generating Itemsets for : 23:30:00\n",
      "Finished generating recommendations. Saving to file...\n",
      "Recommendations saved to file!\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# Generate appliance ON data for each time slice of each day\n",
    "apriori_dt = data_extractor(channel_list, channel_data, train_dates, Time)\n",
    "print(\"Apriori input data generated!\")\n",
    "\n",
    "time_itemset_map = dict()\n",
    "time_rules_map = dict()\n",
    "time_channels_map_from_itemsets = dict()\n",
    "time_channels_map_from_rules = dict()\n",
    "\n",
    "time_appliance_map = divide_data_into_time(Time, apriori_dt)\n",
    "\n",
    "for timestamp in list(time_appliance_map.keys()):\n",
    "\n",
    "    print(\"Generating Itemsets for : \" + str(timestamp))\n",
    "    # Generate frequent itemsets\n",
    "    time_itemset_map[timestamp] = get_support_and_itemsets(time_appliance_map[timestamp], min_support)\n",
    "\n",
    "    # Generate rules\n",
    "    time_rules_map[timestamp] = association_rules(time_itemset_map[timestamp], metric=\"confidence\", min_threshold = min_confidence)\n",
    "\n",
    "    # Filter rules which starts from current time slice\n",
    "    rules_df = time_rules_map[timestamp]\n",
    "    time_rules_map[timestamp] = rules_df[rules_df['antecedents'] == frozenset({timestamp})]\n",
    "\n",
    "    # Get channels from frequent itemsets\n",
    "    time_channels_map_from_itemsets[timestamp] = get_channels_from_frequent_itemsets(time_itemset_map[timestamp])\n",
    "\n",
    "    # Get channels from rules\n",
    "    time_channels_map_from_rules[timestamp] = get_channels_from_rules(time_rules_map[timestamp])\n",
    "\n",
    "    # Save the rules in CSV files to a rules directory\n",
    "    for timestamp, df in time_rules_map.items():\n",
    "        df.to_csv(rule_files_dir + timestamp + \".csv\", header=True, index=False)\n",
    "\n",
    "# Generate Recommendations and save them to a dataframe\n",
    "recommendation_list = []\n",
    "for timestamp in Time:\n",
    "    rule_df = time_rules_map[timestamp].sort_values(by=['confidence'], ascending=False)\n",
    "    recommended_channels = get_channels_from_rules(rule_df[:considered_rules])\n",
    "#         recommendations = get_appliances_from_channels(recommended_channels, labels_df)\n",
    "#         recommendations = change_names(recommendations, show_name_dict)\n",
    "    recommendation_list.append(\",\".join(appliance for appliance in recommended_channels))\n",
    "time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})\n",
    "print(\"Finished generating recommendations. Saving to file...\")\n",
    "\n",
    "time_recommendation_df.to_csv(recommendations_file)\n",
    "print(\"Recommendations saved to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_channel_dict = get_ground_truth(test_dates, Time, channel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_df = pd.read_csv(recommendations_file, header=0, names=['Time','Recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>channel_18,channel_14,channel_6,channel_2,chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>channel_18,channel_14,channel_6,channel_4,chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>channel_18,channel_14,channel_6,channel_4,chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>01:30:00</td>\n",
       "      <td>channel_18,channel_14,channel_6,channel_4,chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>channel_18,channel_14,channel_6,channel_4,chan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time                                    Recommendations\n",
       "0  00:00:00  channel_18,channel_14,channel_6,channel_2,chan...\n",
       "1  00:30:00  channel_18,channel_14,channel_6,channel_4,chan...\n",
       "2  01:00:00  channel_18,channel_14,channel_6,channel_4,chan...\n",
       "3  01:30:00  channel_18,channel_14,channel_6,channel_4,chan...\n",
       "4  02:00:00  channel_18,channel_14,channel_6,channel_4,chan..."
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_recs_dict = dict()\n",
    "for date in Dates:\n",
    "    for index,row in recs_df.iterrows():\n",
    "        time = row['Time']\n",
    "        recommendations = row['Recommendations']    \n",
    "        datetime_recs_dict[date + \" \" + time] = set(list(recommendations.split(\",\")[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_recs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3168032087386939\n"
     ]
    }
   ],
   "source": [
    "average_precision = calculate_precision(test_dates, Time, datetime_channel_dict, datetime_recs_dict)\n",
    "print(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45154591199589844\n"
     ]
    }
   ],
   "source": [
    "average_recall = calculate_recall(test_dates, Time, datetime_channel_dict, datetime_recs_dict)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3723598816027125\n"
     ]
    }
   ],
   "source": [
    "average_f1_score = (2 * average_precision * average_recall)/(average_precision + average_recall)\n",
    "print(average_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Precision', 'Recall', 'F1-score']\n",
    "students = [0.30, 0.97, 0.46]\n",
    "ax.bar(langs,students, width = 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPa8mxrUaeiUkqeP8Zwhgsv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "APRIORI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
