{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8xEFYl3SYfo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels_file_path):\n",
    "    \"\"\"\n",
    "    This method gets channel labels from the labels.dat file from the house directory\n",
    "    \n",
    "    Input:\n",
    "    labels_file_path = Path to the labels.dat file.\n",
    "    \n",
    "    Output:\n",
    "    labels_df = Channel and appliance name dataframe.\n",
    "    labels_dict = Channel and appliance name dictionary.\n",
    "    \"\"\"\n",
    "    labels_df = pd.read_csv(labels_file_path, sep='\\\\s+', names=['Channel_id', 'Appliance'])\n",
    "    labels_df[\"Channel_id\"] = [\"channel_\"+str(i) for i in range(1,labels_df.shape[0]+1)]\n",
    "    labels_dict = dict()\n",
    "    for row in labels_df.iterrows():\n",
    "        labels_dict[row[1][\"Channel_id\"]] = row[1][\"Appliance\"]\n",
    "    return labels_df, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_list(house_path):\n",
    "    \"\"\"\n",
    "    This method gets the list of channels from the house directory.\n",
    "    \n",
    "    Input:\n",
    "    house_path = path to the house directory.\n",
    "    \n",
    "    Output:\n",
    "    channel_list = list of all the channels except channel_1 (mains).\n",
    "    \"\"\"\n",
    "    if(house_path[-1] != '/'):\n",
    "        house_path = house_path + '/'\n",
    "    channel_list = []\n",
    "    for item in os.listdir(house_path):\n",
    "        if 'channel_' in item and item != \"channel_1.dat\":\n",
    "            channel_list.append(item[:-4])\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_data(on_off_data_dir_path, channel_list):\n",
    "    \"\"\"\n",
    "    This method loads channel on/off data from .npy files \n",
    "    generated by running 'Resampling_and_generating_appliance_on_off_data' file \n",
    "    \n",
    "    Input:\n",
    "    file_path = Path to the .npy files for all the channels\n",
    "    equipments = List of channels for the house being analyzed\n",
    "    \n",
    "    Output:\n",
    "    equip_dict = Dictionary of equipment and the datetime data when the equipment was turned on\n",
    "    \"\"\"\n",
    "    channel_dict = dict()\n",
    "    for channel in channel_list:\n",
    "        if 'channel_' in channel and channel != \"channel_1.dat\":    \n",
    "            # print( on_off_data_dir_path + channel)\n",
    "            channel_data = np.load(on_off_data_dir_path + channel + \".npy\")\n",
    "            channel_data = list(channel_data)\n",
    "            # Filter out only those instances where appliance is on\n",
    "            channel_data = [ x for x in channel_data if x!= '0']\n",
    "            # Populating dictionary with channel data using channel name as key\n",
    "            channel_dict[channel] = list(channel_data) \n",
    "    return channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_dates(path_to_resampled_channel_data, channel_list):\n",
    "    \"\"\"\n",
    "    This method finds the range of dates in which all the appliances were recorded //\n",
    "    and returns a min and a max date for given appliances. \n",
    "    \n",
    "    Input:\n",
    "    path_to_resampled_channel_data = Path to the resampled data directory\n",
    "    number_of_channels = Total number of channels in the house\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    min_date = The earliest date on which an appliance usage was recorded.\n",
    "    max_date = The latest date on which an appliance usage was recorded.\n",
    "    \"\"\"\n",
    "    min_date = datetime.datetime.max.date()\n",
    "    max_date = datetime.datetime.min.date()\n",
    "\n",
    "    for i in reversed(range(2, len(channel_list) + 1)):\n",
    "        cd = np.load(path_to_resampled_channel_data + \"channel_\"+str(i)+\".npy\")\n",
    "        for item in cd:\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date < min_date):\n",
    "                    min_date = temp_date\n",
    "                break;\n",
    "\n",
    "        for item in reversed(cd):\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date > max_date):\n",
    "                    max_date = temp_date\n",
    "                break;\n",
    "    return min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_list(min_date, max_date):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of date strings in between these dates\n",
    "    \"\"\"\n",
    "    Dates = []\n",
    "    start_dt = min_date\n",
    "    end_dt = max_date\n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        Dates.append(dt.strftime(\"%Y-%m-%d\"))\n",
    "    return Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of dates in between these dates\n",
    "    \"\"\"\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_times_of_day(interval):\n",
    "    \"\"\"\n",
    "    This method generates a list of times of a day seperated by specified interval.\n",
    "    \n",
    "    Input:\n",
    "    interval = The gap between two neighboring time slots\n",
    "    \n",
    "    Output:\n",
    "    Time = List of times seperated by specified interval\n",
    "    \"\"\"\n",
    "    hour = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "    minute = ['00', str(interval)[:-3]]\n",
    "    second = '00'\n",
    "    Time = []\n",
    "    for hr in hour:\n",
    "        for min in minute:\n",
    "            temp = [hr, min, second]\n",
    "            temp = ':'.join(temp)\n",
    "            Time.append(temp)\n",
    "    return Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usage_data_for_day(channel_data_dict, channel_list, date):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular day. \n",
    "    \n",
    "    Input:\n",
    "    channel_data_dict = Dictionary of channel and their on/off data.\n",
    "    channel_list = List of channels for this house\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    \n",
    "    Output:\n",
    "    cleaned_day_data = Dictionary of appliances used that day and their usage (on timings).\n",
    "    \n",
    "    \"\"\"\n",
    "    day_data = dict()\n",
    "    for channel in channel_list:\n",
    "        channel_data = list(channel_data_dict[channel])\n",
    "        channel_data = [ x for x in channel_data if date in x]\n",
    "        day_data[channel] = channel_data\n",
    "    cleaned_day_data = dict()    \n",
    "    for channel in day_data.keys():\n",
    "        if len(day_data[channel]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_day_data[channel] = day_data[channel]\n",
    "    no_of_channel_w = len(cleaned_day_data.keys()) \n",
    "#     print(\"No of channel working on {%s} are :\"%(date),end = \" \")\n",
    "#     print(no_of_channel_w)       \n",
    "    return cleaned_day_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular time of the day. \n",
    "    \n",
    "    Input:\n",
    "    day_data = Dictionary of channel and their on/off data for the day.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    time = Time for which used appliances and their usage needs to be extracted\n",
    "    channel_list = List of channels for this house\n",
    "        \n",
    "    Output:\n",
    "    list = List of channels on for that time of the day.\n",
    "    \n",
    "    \"\"\"\n",
    "    temp_list = [date, time]\n",
    "    temp_time = ' '.join(temp_list)\n",
    "    time_data = dict()\n",
    "    time_data[time] = []\n",
    "    for channel in day_data.keys():\n",
    "        temp_data = list(channel_data_dict[channel])\n",
    "        temp_data = [ x for x in temp_data if temp_time in x]\n",
    "        if len(temp_data)==0:\n",
    "            continue\n",
    "        else:\n",
    "            time_data[channel] = temp_data\n",
    "#     print(\"No of channels working at %s %s are :\"%(date, time),end =\" \")\n",
    "#     print(len(time_data.keys()))       \n",
    "    return list(time_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extractor(channel_list, channel_data_dict, dates, times):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a given date range. \n",
    "    \n",
    "    Input:\n",
    "    channel_list = List of channels for this house.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    dates = Dates for which used appliances and their usage needs to be extracted.\n",
    "    times = Times for which used appliances and their usage needs to be extracted.\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    list = List-of-lists of channels that are On for that period.\n",
    "    \n",
    "    \"\"\"\n",
    "    transactions = []\n",
    "    no_of_ch = len(channel_list)\n",
    "    # Iterate over dates in the date range\n",
    "    for date in dates:\n",
    "        # Get appliance usage data for day\n",
    "        day_data = get_usage_data_for_day(channel_data_dict, channel_list, date)\n",
    "        # Iterate over times in the 24 hour time range\n",
    "        for time in times:\n",
    "            # Get appliances on for time\n",
    "            temp_list = get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list)           \n",
    "            if len(temp_list)==0:\n",
    "                continue\n",
    "            else:\n",
    "                transactions.append(temp_list)\n",
    "    return transactions           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data_into_time(times, apriori_data):\n",
    "    \"\"\"\n",
    "    This method seperates ON appliance sequences by time and Saves them in a dictionary with time slices as the key\n",
    "    \n",
    "    Input:\n",
    "    times = List of time slices.\n",
    "    apriori_data = Output of the data_extractor().\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets.\n",
    "    \"\"\"\n",
    "    time_sequence_dict = dict()\n",
    "    for timestamp in times:\n",
    "        sequence_list = []\n",
    "        for data in apriori_data:\n",
    "            if timestamp in data:\n",
    "                sequence_list.append(data)\n",
    "        time_sequence_dict[timestamp] = sequence_list\n",
    "    return time_sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_and_itemsets(apriori_data, minimum_support):\n",
    "    \"\"\"\n",
    "    This method runs the apriori algorithm on the input data and returns frequent itemsets with their respective supports\n",
    "    \n",
    "    Input:\n",
    "    apriori_data = List-of-lists of appliances On at a particular time for each day.\n",
    "    minimum_support = Minimum support for getting the frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    data = te.fit(apriori_data).transform(apriori_data)\n",
    "    data = pd.DataFrame(data, columns = te.columns_)\n",
    "    return apriori(data, min_support = minimum_support, use_colnames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_frequent_itemsets(frequent_itemsets_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the support-frequent itemsets dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    frequent_itemsets_df = Dataframe of support and frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the entire frequent itemsets\n",
    "    \"\"\"\n",
    "    channels = set()\n",
    "    for item in frequent_itemsets_df.itemsets:\n",
    "        for entry in list(item):\n",
    "            channels.add(entry)\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appliances_from_channels(list_of_channels, labels_df):\n",
    "    \"\"\"\n",
    "    This method returns a list of appliance names given a list of channels.\n",
    "    \n",
    "    Input:\n",
    "    list_of_channels = List of channels for which names to be extracted.\n",
    "    labels_df = Dataframe of channel ids and appliance names.\n",
    "    \n",
    "    Output:\n",
    "    List of appliance names corresponding to each channel in list_of_channels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in list_of_channels:\n",
    "        result.append(labels_df.set_index('Channel_id').at[item, 'Appliance'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_rules(rule_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_list = []\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = str(sequence)[12:-3].replace(\"'\",'').split(\", \")\n",
    "        for item in list_of_channels:\n",
    "            if item not in channel_list:\n",
    "                channel_list.append(item)\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_channel_list(rule_df, full_channel_set):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels not present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    full_channel_set = Set of complete channels in the house\n",
    "    \n",
    "    Output:\n",
    "    Set of channels not present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_set = set()\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = sequence[12:-3].replace(\"'\",'').split(\", \")\n",
    "        channel_set.update(list_of_channels)\n",
    "    result_1 = full_channel_set - channel_set\n",
    "    return result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(recs, show_name_dict):\n",
    "    res = []\n",
    "    for item in recs:\n",
    "        res.append(show_name_dict[item])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_names_dict(labels_to_name_file):\n",
    "    display_names_dict = dict()\n",
    "    display_names_df = pd.read_csv(labels_to_name_file, names= [\"Labels\", \"Name\"], header = 0)\n",
    "    for index, row in display_names_df.iterrows():\n",
    "        display_names_dict[row.Labels] = row.Name\n",
    "    return display_names_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CHANGE SHOW_NAME_DICT TO DISPLAY_NAME_DICT\n",
    "\n",
    "def convert_rule_mining_recs_to_csv(recs_df, show_name_dict):\n",
    "    recommendation_list = []\n",
    "    for item in recs_df['Recommendations']:\n",
    "        recs = []\n",
    "        item = item[1:-1]\n",
    "        for channel in item.split(\", \"):\n",
    "            channel = channel[2:-2]\n",
    "            recs.append(channel)\n",
    "        recommendations = get_appliances_from_channels(recs, labels_df)\n",
    "        recommendations = change_names(recommendations, show_name_dict)\n",
    "        recommendation_list.append(\",\".join(appliance for appliance in recommendations))\n",
    "    time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})\n",
    "    return time_recommendation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House : 2\n",
      "path_to_house : ../../../../Dataset/ukdale/House_2/\n",
      "labels_file : ../../../../Dataset/ukdale/House_2/labels.dat\n",
      "labels_to_name_file : ../Labels_to_name_files/House_2.csv\n",
      "channel_status_file_dir : ../Utils/Channel_On_Off_data/House_2/\n",
      "apriori_data_output_file : ../Utils/Channel_On_Off_data/House_2/apriori_data.py\n",
      "rule_files_dir : ./Rule_Files/House_2/\n",
      "recommendations_file : ./Recommendations/House_2/Recommendations.csv\n",
      "resampling_time_in_min : 30min\n",
      "min_support : 0.02\n",
      "min_confidence : 0.02\n",
      "considered_rules : 200\n"
     ]
    }
   ],
   "source": [
    "house = 2\n",
    "\n",
    "print(\"House : \" + str(house))\n",
    "\n",
    "path_to_house = \"../../../../Dataset/ukdale/House_\" + str(house) + \"/\"\n",
    "labels_file_path = path_to_house + \"labels.dat\"\n",
    "labels_to_name_file = \"../Labels_to_name_files/House_\" + str(house) + \".csv\"\n",
    "channel_status_file_dir = \"../Utils/Channel_On_Off_data/House_\" + str(house) + \"/\"\n",
    "apriori_data_output_file = channel_status_file_dir + \"apriori_data.py\"\n",
    "rule_files_dir = \"./Rule_Files/House_\" + str(house) + \"/\"\n",
    "# recommendations_file = \"./Recommendations/House_\" + str(house) + \"/recommendations_\" + str(min_support) + \"_\" + str(min_confidence) + \".csv\"\n",
    "recommendations_file = \"./Recommendations/House_\" + str(house) + \"/Recommendations.csv\"\n",
    "\n",
    "resampling_time_in_min = \"30min\"\n",
    "min_support = 0.02\n",
    "min_confidence = 0.02\n",
    "considered_rules = 200\n",
    "channel_list = get_channel_list(path_to_house)\n",
    "\n",
    "print(\"path_to_house : \" + path_to_house)\n",
    "print(\"labels_file : \" + labels_file_path)\n",
    "print(\"labels_to_name_file : \" + labels_to_name_file)\n",
    "print(\"channel_status_file_dir : \" + channel_status_file_dir)\n",
    "print(\"apriori_data_output_file : \" + apriori_data_output_file)\n",
    "print(\"rule_files_dir : \" + rule_files_dir)\n",
    "print(\"recommendations_file : \" + recommendations_file)\n",
    "\n",
    "print(\"resampling_time_in_min : \" + resampling_time_in_min)\n",
    "print(\"min_support : \" + str(min_support))\n",
    "print(\"min_confidence : \" + str(min_confidence))\n",
    "print(\"considered_rules : \" + str(considered_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_name_dict = get_display_names_dict(labels_to_name_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_names_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels from the labels.dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df, labels_map = get_labels(labels_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_id</th>\n",
       "      <th>Appliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>channel_1</td>\n",
       "      <td>aggregate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>channel_2</td>\n",
       "      <td>laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>channel_3</td>\n",
       "      <td>monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>channel_4</td>\n",
       "      <td>speakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>channel_5</td>\n",
       "      <td>server</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>channel_6</td>\n",
       "      <td>router</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>channel_7</td>\n",
       "      <td>server_hdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>channel_8</td>\n",
       "      <td>kettle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>channel_9</td>\n",
       "      <td>rice_cooker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>channel_10</td>\n",
       "      <td>running_machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>channel_11</td>\n",
       "      <td>laptop2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>channel_12</td>\n",
       "      <td>washing_machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>channel_13</td>\n",
       "      <td>dish_washer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>channel_14</td>\n",
       "      <td>fridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>channel_15</td>\n",
       "      <td>microwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>channel_16</td>\n",
       "      <td>toaster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>channel_17</td>\n",
       "      <td>playstation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>channel_18</td>\n",
       "      <td>modem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>channel_19</td>\n",
       "      <td>cooker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Channel_id        Appliance\n",
       "0    channel_1        aggregate\n",
       "1    channel_2           laptop\n",
       "2    channel_3          monitor\n",
       "3    channel_4         speakers\n",
       "4    channel_5           server\n",
       "5    channel_6           router\n",
       "6    channel_7       server_hdd\n",
       "7    channel_8           kettle\n",
       "8    channel_9      rice_cooker\n",
       "9   channel_10  running_machine\n",
       "10  channel_11          laptop2\n",
       "11  channel_12  washing_machine\n",
       "12  channel_13      dish_washer\n",
       "13  channel_14           fridge\n",
       "14  channel_15        microwave\n",
       "15  channel_16          toaster\n",
       "16  channel_17      playstation\n",
       "17  channel_18            modem\n",
       "18  channel_19           cooker"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get house related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel list and data from channel files\n",
    "channel_list = get_channel_list(channel_status_file_dir)\n",
    "channel_data = get_channel_data(channel_status_file_dir, channel_list)\n",
    "\n",
    "# Get max and min date in the channel data\n",
    "min_date, max_date = get_min_max_dates(channel_status_file_dir, channel_list)\n",
    "\n",
    "# Generate date and time list \n",
    "Dates = get_dates_list(min_date, max_date)\n",
    "Time = get_all_times_of_day(resampling_time_in_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract appliance usage data from the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate appliance ON data for each time slice of each day and save it.\n",
    "apriori_dt = data_extractor(channel_list, channel_data, Dates, Time)\n",
    "np.save(apriori_data_output_file, apriori_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(apriori_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate rules for each time of the day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Itemsets for : 00:00:00\n",
      "Generating Itemsets for : 00:30:00\n",
      "Generating Itemsets for : 01:00:00\n",
      "Generating Itemsets for : 01:30:00\n",
      "Generating Itemsets for : 02:00:00\n",
      "Generating Itemsets for : 02:30:00\n",
      "Generating Itemsets for : 03:00:00\n",
      "Generating Itemsets for : 03:30:00\n",
      "Generating Itemsets for : 04:00:00\n",
      "Generating Itemsets for : 04:30:00\n",
      "Generating Itemsets for : 05:00:00\n",
      "Generating Itemsets for : 05:30:00\n",
      "Generating Itemsets for : 06:00:00\n",
      "Generating Itemsets for : 06:30:00\n",
      "Generating Itemsets for : 07:00:00\n",
      "Generating Itemsets for : 07:30:00\n",
      "Generating Itemsets for : 08:00:00\n",
      "Generating Itemsets for : 08:30:00\n",
      "Generating Itemsets for : 09:00:00\n",
      "Generating Itemsets for : 09:30:00\n",
      "Generating Itemsets for : 10:00:00\n",
      "Generating Itemsets for : 10:30:00\n",
      "Generating Itemsets for : 11:00:00\n",
      "Generating Itemsets for : 11:30:00\n",
      "Generating Itemsets for : 12:00:00\n",
      "Generating Itemsets for : 12:30:00\n",
      "Generating Itemsets for : 13:00:00\n",
      "Generating Itemsets for : 13:30:00\n",
      "Generating Itemsets for : 14:00:00\n",
      "Generating Itemsets for : 14:30:00\n",
      "Generating Itemsets for : 15:00:00\n",
      "Generating Itemsets for : 15:30:00\n",
      "Generating Itemsets for : 16:00:00\n",
      "Generating Itemsets for : 16:30:00\n",
      "Generating Itemsets for : 17:00:00\n",
      "Generating Itemsets for : 17:30:00\n",
      "Generating Itemsets for : 18:00:00\n",
      "Generating Itemsets for : 18:30:00\n",
      "Generating Itemsets for : 19:00:00\n",
      "Generating Itemsets for : 19:30:00\n",
      "Generating Itemsets for : 20:00:00\n",
      "Generating Itemsets for : 20:30:00\n",
      "Generating Itemsets for : 21:00:00\n",
      "Generating Itemsets for : 21:30:00\n",
      "Generating Itemsets for : 22:00:00\n",
      "Generating Itemsets for : 22:30:00\n",
      "Generating Itemsets for : 23:00:00\n",
      "Generating Itemsets for : 23:30:00\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# Generate frequent itemsets and rules one time slice at a time and save them in a dictionary. \n",
    "\n",
    "time_itemset_map = dict()\n",
    "time_rules_map = dict()\n",
    "time_channels_map_from_itemsets = dict()\n",
    "time_channels_map_from_rules = dict()\n",
    "\n",
    "time_appliance_map = divide_data_into_time(Time, apriori_dt)\n",
    "\n",
    "for timestamp in list(time_appliance_map.keys()):\n",
    "    \n",
    "    print(\"Generating Itemsets for : \" + str(timestamp))\n",
    "    # Generate frequent itemsets\n",
    "    time_itemset_map[timestamp] = get_support_and_itemsets(time_appliance_map[timestamp], min_support)\n",
    "    \n",
    "#     print(\"Generating Rules for : \" + str(timestamp))\n",
    "    # Generate rules\n",
    "    time_rules_map[timestamp] = association_rules(time_itemset_map[timestamp], metric=\"confidence\", min_threshold = min_confidence)\n",
    "#     print(time_rules_map[timestamp].shape)\n",
    "    \n",
    "#     print(\"Filtering Rules for : \" + str(timestamp))\n",
    "    # Filter rules which starts from current time slice\n",
    "    rules_df = time_rules_map[timestamp]\n",
    "    time_rules_map[timestamp] = rules_df[rules_df['antecedents'] == frozenset({timestamp})]\n",
    "    \n",
    "    # Get channels from frequent itemsets\n",
    "    time_channels_map_from_itemsets[timestamp] = get_channels_from_frequent_itemsets(time_itemset_map[timestamp])\n",
    "    \n",
    "    # Get channels from rules\n",
    "    time_channels_map_from_rules[timestamp] = get_channels_from_rules(time_rules_map[timestamp])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the rules in CSV files to a rules directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp, df in time_rules_map.items():\n",
    "    df.to_csv(rule_files_dir + timestamp + \".csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Recommendations and save them to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_list = []\n",
    "for timestamp in Time:\n",
    "    rule_df = time_rules_map[timestamp].sort_values(by=['confidence'], ascending=False)\n",
    "    recommended_channels = get_channels_from_rules(rule_df[:considered_rules])\n",
    "    recommendations = get_appliances_from_channels(recommended_channels, labels_df)\n",
    "    recommendations = change_names(recommendations, show_name_dict)\n",
    "    recommendation_list.append(\",\".join(appliance for appliance in recommendations))\n",
    "time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_recommendation_df.to_csv(recommendations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apriori_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPa8mxrUaeiUkqeP8Zwhgsv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "APRIORI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
