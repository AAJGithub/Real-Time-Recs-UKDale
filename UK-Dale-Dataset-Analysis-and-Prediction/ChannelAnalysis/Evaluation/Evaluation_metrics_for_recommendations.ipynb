{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8xEFYl3SYfo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(labels_file_path):\n",
    "    \"\"\"\n",
    "    This method gets channel labels from the labels.dat file from the house directory\n",
    "    \n",
    "    Input:\n",
    "    labels_file_path = Path to the labels.dat file.\n",
    "    \n",
    "    Output:\n",
    "    labels_df = Channel and appliance name dataframe.\n",
    "    labels_dict = Channel and appliance name dictionary.\n",
    "    \"\"\"\n",
    "    labels_df = pd.read_csv(labels_file_path, sep='\\\\s+', names=['Channel_id', 'Appliance'])\n",
    "    labels_df[\"Channel_id\"] = [\"channel_\"+str(i) for i in range(1,labels_df.shape[0]+1)]\n",
    "    labels_dict = dict()\n",
    "    for row in labels_df.iterrows():\n",
    "        labels_dict[row[1][\"Channel_id\"]] = row[1][\"Appliance\"]\n",
    "    return labels_df, labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_list(house_path):\n",
    "    \"\"\"\n",
    "    This method gets the list of channels from the house directory.\n",
    "    \n",
    "    Input:\n",
    "    house_path = path to the house directory.\n",
    "    \n",
    "    Output:\n",
    "    channel_list = list of all the channels except channel_1 (mains).\n",
    "    \"\"\"\n",
    "    if(house_path[-1] != '/'):\n",
    "        house_path = house_path + '/'\n",
    "    channel_list = []\n",
    "    for item in os.listdir(path):\n",
    "        if 'channel_' in item and item != \"channel_1.dat\":\n",
    "            channel_list.append(item[:-4])\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJOlnYKdUlio"
   },
   "outputs": [],
   "source": [
    "def get_channel_data(on_off_data_dir_path, channel_list):\n",
    "    \"\"\"\n",
    "    This method loads channel on/off data from .npy files \n",
    "    generated by running 'Resampling_and_generating_appliance_on_off_data' file \n",
    "    \n",
    "    Input:\n",
    "    file_path = Path to the .npy files for all the channels\n",
    "    equipments = List of channels for the house being analyzed\n",
    "    \n",
    "    Output:\n",
    "    equip_dict = Dictionary of equipment and the datetime data when the equipment was turned on\n",
    "    \"\"\"\n",
    "    channel_dict = dict()\n",
    "    for channel in channel_list:\n",
    "        if 'channel_' in channel and channel != \"channel_1.dat\":    \n",
    "            # print( on_off_data_dir_path + channel)\n",
    "            channel_data = np.load(on_off_data_dir_path + channel + \".npy\")\n",
    "            channel_data = list(channel_data)\n",
    "            # Filter out only those instances where appliance is on\n",
    "            channel_data = [ x for x in channel_data if x!= '0']\n",
    "            # Populating dictionary with channel data using channel name as key\n",
    "            channel_dict[channel] = list(channel_data) \n",
    "    return channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_dates(path_to_resampled_channel_data, channel_list):\n",
    "    \"\"\"\n",
    "    This method finds the range of dates in which all the appliances were recorded //\n",
    "    and returns a min and a max date for given appliances. \n",
    "    \n",
    "    Input:\n",
    "    path_to_resampled_channel_data = Path to the resampled data directory\n",
    "    number_of_channels = Total number of channels in the house\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    min_date = The earliest date on which an appliance usage was recorded.\n",
    "    max_date = The latest date on which an appliance usage was recorded.\n",
    "    \"\"\"\n",
    "    min_date = datetime.datetime.max.date()\n",
    "    max_date = datetime.datetime.min.date()\n",
    "\n",
    "    for i in reversed(range(2, len(channel_list) + 1)):\n",
    "        cd = np.load(path_to_resampled_channel_data + \"channel_\"+str(i)+\".npy\")\n",
    "        for item in cd:\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date < min_date):\n",
    "                    min_date = temp_date\n",
    "                break;\n",
    "\n",
    "        for item in reversed(cd):\n",
    "            if(item != '0'):\n",
    "                datetime_obj = datetime.datetime.strptime(item, \"%Y-%m-%d %H:%M:%S\")\n",
    "                temp_date = datetime_obj.date()\n",
    "                if(temp_date > max_date):\n",
    "                    max_date = temp_date\n",
    "                break;\n",
    "    return min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates_list(min_date, max_date):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of date strings in between these dates\n",
    "    \"\"\"\n",
    "    Dates = []\n",
    "    start_dt = min_date\n",
    "    end_dt = max_date\n",
    "    for dt in daterange(start_dt, end_dt):\n",
    "        Dates.append(dt.strftime(\"%Y-%m-%d\"))\n",
    "    return Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    \"\"\"\n",
    "    This method finds the dates between a given set of dates.\n",
    "    \n",
    "    Input:\n",
    "    date1 = Start date\n",
    "    date2 = End date\n",
    "    \n",
    "    Output:\n",
    "    List of dates in between these dates\n",
    "    \"\"\"\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kM4-_W0udTzB"
   },
   "outputs": [],
   "source": [
    "def get_all_times_of_day(interval):\n",
    "    \"\"\"\n",
    "    This method generates a list of times of a day seperated by specified interval.\n",
    "    \n",
    "    Input:\n",
    "    interval = The gap between two neighboring time slots\n",
    "    \n",
    "    Output:\n",
    "    Time = List of times seperated by specified interval\n",
    "    \"\"\"\n",
    "    hour = ['00','01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23']\n",
    "    minute = ['00', str(interval)]\n",
    "    second = '00'\n",
    "    Time = []\n",
    "    for hr in hour:\n",
    "        for min in minute:\n",
    "            temp = [hr, min, second]\n",
    "            temp = ':'.join(temp)\n",
    "            Time.append(temp)\n",
    "    return Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcIT8idqnM4O"
   },
   "outputs": [],
   "source": [
    "def get_usage_data_for_day(channel_data_dict, channel_list, date):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular day. \n",
    "    \n",
    "    Input:\n",
    "    channel_data_dict = Dictionary of channel and their on/off data.\n",
    "    channel_list = List of channels for this house\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    \n",
    "    Output:\n",
    "    cleaned_day_data = Dictionary of appliances used that day and their usage (on timings).\n",
    "    \n",
    "    \"\"\"\n",
    "    day_data = dict()\n",
    "    for channel in channel_list:\n",
    "        channel_data = list(channel_data_dict[channel])\n",
    "        channel_data = [ x for x in channel_data if date in x]\n",
    "        day_data[channel] = channel_data\n",
    "    cleaned_day_data = dict()    \n",
    "    for channel in day_data.keys():\n",
    "        if len(day_data[channel]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_day_data[channel] = day_data[channel]\n",
    "    no_of_channel_w = len(cleaned_day_data.keys()) \n",
    "#     print(\"No of channel working on {%s} are :\"%(date),end = \" \")\n",
    "#     print(no_of_channel_w)       \n",
    "    return cleaned_day_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unVB8HYd6-uJ"
   },
   "outputs": [],
   "source": [
    "def get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a particular time of the day. \n",
    "    \n",
    "    Input:\n",
    "    day_data = Dictionary of channel and their on/off data for the day.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    date = Date for which used appliances and their usage needs to be extracted\n",
    "    time = Time for which used appliances and their usage needs to be extracted\n",
    "    channel_list = List of channels for this house\n",
    "        \n",
    "    Output:\n",
    "    list = List of channels on for that time of the day.\n",
    "    \n",
    "    \"\"\"\n",
    "    temp_list = [date, time]\n",
    "    temp_time = ' '.join(temp_list)\n",
    "    time_data = dict()\n",
    "    time_data[time] = []\n",
    "    for channel in day_data.keys():\n",
    "        temp_data = list(channel_data_dict[channel])\n",
    "        temp_data = [ x for x in temp_data if temp_time in x]\n",
    "        if len(temp_data)==0:\n",
    "            continue\n",
    "        else:\n",
    "            time_data[channel] = temp_data\n",
    "#     print(\"No of channels working at %s %s are :\"%(date, time),end =\" \")\n",
    "#     print(len(time_data.keys()))       \n",
    "    return list(time_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AiUiRUO4e2qE"
   },
   "outputs": [],
   "source": [
    "def data_extractor(channel_list, channel_data_dict, dates, times):\n",
    "    \"\"\"\n",
    "    This method extracts the appliances that are on and their usage data for a given date range. \n",
    "    \n",
    "    Input:\n",
    "    channel_list = List of channels for this house.\n",
    "    channel_data_dict = Complete dictionary of channel and their on/off data.\n",
    "    dates = Dates for which used appliances and their usage needs to be extracted.\n",
    "    times = Times for which used appliances and their usage needs to be extracted.\n",
    "    \n",
    "        \n",
    "    Output:\n",
    "    list = List-of-lists of channels that are On for that period.\n",
    "    \n",
    "    \"\"\"\n",
    "    transactions = []\n",
    "    no_of_ch = len(channel_list)\n",
    "    # Iterate over dates in the date range\n",
    "    for date in dates:\n",
    "        # Get appliance usage data for day\n",
    "        day_data = get_usage_data_for_day(channel_data_dict, channel_list, date)\n",
    "        # Iterate over times in the 24 hour time range\n",
    "        for time in times:\n",
    "            # Get appliances on for time\n",
    "            temp_list = get_usage_data_for_time(day_data, channel_data_dict, date, time, channel_list)           \n",
    "            if len(temp_list)==0:\n",
    "                continue\n",
    "            else:\n",
    "                transactions.append(temp_list)\n",
    "    return transactions                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data_into_time(times, apriori_data):\n",
    "    \"\"\"\n",
    "    This method seperates ON appliance sequences by time and Saves them in a dictionary with time slices as the key\n",
    "    \n",
    "    Input:\n",
    "    times = List of time slices.\n",
    "    apriori_data = Output of the data_extractor().\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets.\n",
    "    \"\"\"\n",
    "    time_sequence_dict = dict()\n",
    "    for timestamp in times:\n",
    "        sequence_list = []\n",
    "        for data in apriori_data:\n",
    "            if timestamp in data:\n",
    "                sequence_list.append(data)\n",
    "        time_sequence_dict[timestamp] = sequence_list\n",
    "    return time_sequence_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_support_and_itemsets(apriori_data, minimum_support):\n",
    "    \"\"\"\n",
    "    This method runs the apriori algorithm on the input data and returns frequent itemsets with their respective supports\n",
    "    \n",
    "    Input:\n",
    "    apriori_data = List-of-lists of appliances On at a particular time for each day.\n",
    "    minimum_support = Minimum support for getting the frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Dataframe of support and frequent itemsets\n",
    "    \"\"\"\n",
    "    te = TransactionEncoder()\n",
    "    data = te.fit(apriori_data).transform(apriori_data)\n",
    "    data = pd.DataFrame(data, columns = te.columns_)\n",
    "    return apriori(data, min_support = minimum_support, use_colnames = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_frequent_itemsets(frequent_itemsets_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the support-frequent itemsets dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    frequent_itemsets_df = Dataframe of support and frequent itemsets.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the entire frequent itemsets\n",
    "    \"\"\"\n",
    "    channels = set()\n",
    "    for item in frequent_itemsets_df.itemsets:\n",
    "        for entry in list(item):\n",
    "            channels.add(entry)\n",
    "    return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_appliances_from_channels(list_of_channels, labels_df):\n",
    "    \"\"\"\n",
    "    This method returns a list of appliance names given a list of channels.\n",
    "    \n",
    "    Input:\n",
    "    list_of_channels = List of channels for which names to be extracted.\n",
    "    labels_df = Dataframe of channel ids and appliance names.\n",
    "    \n",
    "    Output:\n",
    "    List of appliance names corresponding to each channel in list_of_channels\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for item in list_of_channels:\n",
    "        result.append(labels_df.set_index('Channel_id').at[item, 'Appliance'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channels_from_rules(rule_df):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    \n",
    "    Output:\n",
    "    Set of channels present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_list = []\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = str(sequence)[12:-3].replace(\"'\",'').split(\", \")\n",
    "        for item in list_of_channels:\n",
    "            if item not in channel_list:\n",
    "                channel_list.append(item)\n",
    "    return channel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_channel_list(rule_df, full_channel_set):\n",
    "    \"\"\"\n",
    "    This method returns the set of unique channels not present in the time-rules dataframe.\n",
    "    This method was used to check the appliance coverage.\n",
    "    \n",
    "    Input:\n",
    "    rules_df = Dataframe of time and appliances On.\n",
    "    full_channel_set = Set of complete channels in the house\n",
    "    \n",
    "    Output:\n",
    "    Set of channels not present in the rules dataframe\n",
    "    \"\"\"\n",
    "    channel_set = set()\n",
    "    for sequence in rule_df['consequents']:\n",
    "        list_of_channels = sequence[12:-3].replace(\"'\",'').split(\", \")\n",
    "        channel_set.update(list_of_channels)\n",
    "    result_1 = full_channel_set - channel_set\n",
    "    return result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_names(recs, show_name_dict):\n",
    "    res = []\n",
    "    for item in recs:\n",
    "        res.append(show_name_dict[item])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_janhavis_recs_to_csv(recs_df):\n",
    "    recommendation_list = []\n",
    "    for item in recs_df['Recommendations']:\n",
    "        recs = []\n",
    "        item = item[1:-1]\n",
    "        for channel in item.split(\", \"):\n",
    "            channel = channel[2:-2]\n",
    "            recs.append(channel)\n",
    "        recommendations = get_appliances_from_channels(recs, labels_df)\n",
    "        recommendations = change_names(recommendations, show_name_dict)\n",
    "        recommendation_list.append(\",\".join(appliance for appliance in recommendations))\n",
    "    time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})\n",
    "    return time_recommendation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(Dates, Time, channel_data):\n",
    "    datetime_channel_dict = dict()\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            datetime_channel_dict[date + \" \" + timestamp] = set()\n",
    "            for channel in channel_data.keys():\n",
    "                if(date + \" \" + timestamp in channel_data[channel]):\n",
    "                    datetime_channel_dict[date + \" \" + timestamp].add(channel)\n",
    "    return datetime_channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_recommendations(k, Dates, recs_file_path):\n",
    "    datetime_recs_dict = dict()\n",
    "    recs_df = pd.read_csv(recs_file_path, header=0, names=['Time','Recommendations'])\n",
    "    for date in Dates:\n",
    "        for index,row in recs_df.iterrows():\n",
    "            time = row['Time']\n",
    "            recommendations = row['Recommendations']    \n",
    "            datetime_recs_dict[date + \" \" + time] = set(list(recommendations.split(\",\")[:k]))\n",
    "    return datetime_recs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(Dates, Time, ground_truth, recommendations):\n",
    "    count = 0\n",
    "    precision = 0\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            relevant = ground_truth[date + \" \" + timestamp]\n",
    "            recommended = recommendations[date + \" \" + timestamp]\n",
    "            if(len(relevant) == 0):\n",
    "                continue\n",
    "            count += 1\n",
    "            temp_precision = (len(relevant.intersection(recommended))/len(recommended))\n",
    "            precision += temp_precision\n",
    "    avg_precision = precision/count\n",
    "    return avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(Dates, Time, ground_truth, recommendations):\n",
    "    count = 0\n",
    "    recall = 0\n",
    "    for date in Dates:\n",
    "        for timestamp in Time:\n",
    "            relevant = ground_truth[date + \" \" + timestamp]\n",
    "            recommended = recommendations[date + \" \" + timestamp]\n",
    "            if(len(relevant) == 0):\n",
    "                continue\n",
    "            count += 1\n",
    "            temp_recall = (len(relevant.intersection(recommended))/len(relevant))\n",
    "            recall += temp_recall\n",
    "    avg_recall = recall/count\n",
    "    return avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare necessary variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaggregated data\n",
    "\n",
    "# path = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/\"\n",
    "# resampling_time_in_min = '30'\n",
    "# min_confidence = 0.4\n",
    "# min_support = 0.1\n",
    "# considered_rules = 200\n",
    "# labels_file = \"../../Disaggregation/House_2/labels.dat\"\n",
    "# apriori_data_output_file = \"../Utils/Channel_On_Off_data/Disaggregated_house_2/apriori_data.npy\"\n",
    "# rule_files_dir = \"./Disaggregated_Rule_Files/\"\n",
    "# recommendations_file = './disaggregated_recommendations_01_04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 2 Lables\n",
    "\n",
    "# show_name_dict = dict()\n",
    "# show_name_dict['laptop'] = \"Laptop\"\n",
    "# show_name_dict['monitor'] = \"Monitor\"\n",
    "# show_name_dict['speakers'] = \"Speakers\"\n",
    "# show_name_dict['server'] = \"Server\"\n",
    "# show_name_dict['router'] = \"Router\"\n",
    "# show_name_dict['server_hdd'] = \"Server_hdd\"\n",
    "# show_name_dict['kettle'] = \"Kettle\"\n",
    "# show_name_dict['rice_cooker'] = \"Rice Cooker\"\n",
    "# show_name_dict['running_machine'] = \"Running Machine\"\n",
    "# show_name_dict['laptop2'] = \"Laptop2\"\n",
    "# show_name_dict['washing_machine'] = \"Washing Machine\"\n",
    "# show_name_dict['dish_washer'] = \"Dish Washer\"\n",
    "# show_name_dict['fridge'] = \"Fridge\"\n",
    "# show_name_dict['microwave'] = \"Microwave\"\n",
    "# show_name_dict['toaster'] = \"Toaster\"\n",
    "# show_name_dict['playstation'] = \"Playstation\"\n",
    "# show_name_dict['modem'] = \"Modem\"\n",
    "# show_name_dict['cooker'] = \"Cooker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "house = 'House_3'\n",
    "min_support = 0.02\n",
    "min_confidence = 0.02\n",
    "considered_rules = 200\n",
    "resampling_time_in_min = '30'\n",
    "path = \"../Utils/Channel_On_Off_data/\" + house + \"/\"\n",
    "labels_file = \"../../../../Dataset/ukdale/\" + house + \"/labels.dat\"\n",
    "rule_files_dir = \"./Train/Rule_Files/\" + house + \"/\"\n",
    "recommendations_file = \"./Train/Recommendations/\" + house + \"/recommendations_\" + str(min_support) + \"_\" + str(min_confidence) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 3 Lables\n",
    "\n",
    "show_name_dict = dict()\n",
    "show_name_dict['laptop'] = \"Laptop\"\n",
    "show_name_dict['kettle'] = \"Kettle\"\n",
    "show_name_dict['electric_heater'] = \"Electric Heater\"\n",
    "show_name_dict['projector'] = \"Projector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# House 4 Lables\n",
    "\n",
    "# show_name_dict = dict()\n",
    "# show_name_dict['tv_dvd_digibox_lamp'] = \"TV & DVD Digibox & Lamp\"\n",
    "# show_name_dict['kettle_radio'] = \"Kettle & Radio\"\n",
    "# show_name_dict['gas_boiler'] = \"Gas Boiler\"\n",
    "# show_name_dict['freezer'] = \"Freezer\"\n",
    "# show_name_dict['washing_machine_microwave_breadmaker'] = \"Washing Machine & Microwave & Breadmaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels from the labels.dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df, labels_map = get_labels(labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel_id</th>\n",
       "      <th>Appliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>channel_1</td>\n",
       "      <td>aggregate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>channel_2</td>\n",
       "      <td>kettle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>channel_3</td>\n",
       "      <td>electric_heater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>channel_4</td>\n",
       "      <td>laptop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>channel_5</td>\n",
       "      <td>projector</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Channel_id        Appliance\n",
       "0  channel_1        aggregate\n",
       "1  channel_2           kettle\n",
       "2  channel_3  electric_heater\n",
       "3  channel_4           laptop\n",
       "4  channel_5        projector"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get house related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get channel list and data from channel files\n",
    "channel_list = get_channel_list(path)\n",
    "channel_data = get_channel_data(path, channel_list)\n",
    "\n",
    "# Get max and min date in the channel data\n",
    "min_date, max_date = get_min_max_dates(path, channel_list)\n",
    "\n",
    "# Generate date and time list \n",
    "Dates = get_dates_list(min_date, max_date)\n",
    "\n",
    "train_dates = Dates[:round(len(Dates)*0.9)]\n",
    "test_dates = Dates[round(len(Dates)*0.9):]\n",
    "\n",
    "Time = get_all_times_of_day(resampling_time_in_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate rules for each time of the day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori input data generated!\n",
      "Generating Itemsets for : 00:00:00\n",
      "Generating Itemsets for : 00:30:00\n",
      "Generating Itemsets for : 01:00:00\n",
      "Generating Itemsets for : 01:30:00\n",
      "Generating Itemsets for : 02:00:00\n",
      "Generating Itemsets for : 02:30:00\n",
      "Generating Itemsets for : 03:00:00\n",
      "Generating Itemsets for : 03:30:00\n",
      "Generating Itemsets for : 04:00:00\n",
      "Generating Itemsets for : 04:30:00\n",
      "Generating Itemsets for : 05:00:00\n",
      "Generating Itemsets for : 05:30:00\n",
      "Generating Itemsets for : 06:00:00\n",
      "Generating Itemsets for : 06:30:00\n",
      "Generating Itemsets for : 07:00:00\n",
      "Generating Itemsets for : 07:30:00\n",
      "Generating Itemsets for : 08:00:00\n",
      "Generating Itemsets for : 08:30:00\n",
      "Generating Itemsets for : 09:00:00\n",
      "Generating Itemsets for : 09:30:00\n",
      "Generating Itemsets for : 10:00:00\n",
      "Generating Itemsets for : 10:30:00\n",
      "Generating Itemsets for : 11:00:00\n",
      "Generating Itemsets for : 11:30:00\n",
      "Generating Itemsets for : 12:00:00\n",
      "Generating Itemsets for : 12:30:00\n",
      "Generating Itemsets for : 13:00:00\n",
      "Generating Itemsets for : 13:30:00\n",
      "Generating Itemsets for : 14:00:00\n",
      "Generating Itemsets for : 14:30:00\n",
      "Generating Itemsets for : 15:00:00\n",
      "Generating Itemsets for : 15:30:00\n",
      "Generating Itemsets for : 16:00:00\n",
      "Generating Itemsets for : 16:30:00\n",
      "Generating Itemsets for : 17:00:00\n",
      "Generating Itemsets for : 17:30:00\n",
      "Generating Itemsets for : 18:00:00\n",
      "Generating Itemsets for : 18:30:00\n",
      "Generating Itemsets for : 19:00:00\n",
      "Generating Itemsets for : 19:30:00\n",
      "Generating Itemsets for : 20:00:00\n",
      "Generating Itemsets for : 20:30:00\n",
      "Generating Itemsets for : 21:00:00\n",
      "Generating Itemsets for : 21:30:00\n",
      "Generating Itemsets for : 22:00:00\n",
      "Generating Itemsets for : 22:30:00\n",
      "Generating Itemsets for : 23:00:00\n",
      "Generating Itemsets for : 23:30:00\n",
      "Finished generating recommendations. Saving to file...\n",
      "Recommendations saved to file!\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# Generate appliance ON data for each time slice of each day\n",
    "apriori_dt = data_extractor(channel_list, channel_data, train_dates, Time)\n",
    "print(\"Apriori input data generated!\")\n",
    "\n",
    "time_itemset_map = dict()\n",
    "time_rules_map = dict()\n",
    "time_channels_map_from_itemsets = dict()\n",
    "time_channels_map_from_rules = dict()\n",
    "\n",
    "time_appliance_map = divide_data_into_time(Time, apriori_dt)\n",
    "\n",
    "for timestamp in list(time_appliance_map.keys()):\n",
    "\n",
    "    print(\"Generating Itemsets for : \" + str(timestamp))\n",
    "    # Generate frequent itemsets\n",
    "    time_itemset_map[timestamp] = get_support_and_itemsets(time_appliance_map[timestamp], min_support)\n",
    "\n",
    "    # Generate rules\n",
    "    time_rules_map[timestamp] = association_rules(time_itemset_map[timestamp], metric=\"confidence\", min_threshold = min_confidence)\n",
    "\n",
    "    # Filter rules which starts from current time slice\n",
    "    rules_df = time_rules_map[timestamp]\n",
    "    time_rules_map[timestamp] = rules_df[rules_df['antecedents'] == frozenset({timestamp})]\n",
    "\n",
    "    # Get channels from frequent itemsets\n",
    "    time_channels_map_from_itemsets[timestamp] = get_channels_from_frequent_itemsets(time_itemset_map[timestamp])\n",
    "\n",
    "    # Get channels from rules\n",
    "    time_channels_map_from_rules[timestamp] = get_channels_from_rules(time_rules_map[timestamp])\n",
    "\n",
    "    # Save the rules in CSV files to a rules directory\n",
    "    for timestamp, df in time_rules_map.items():\n",
    "        df.to_csv(rule_files_dir + timestamp + \".csv\", header=True, index=False)\n",
    "\n",
    "# Generate Recommendations and save them to a dataframe\n",
    "recommendation_list = []\n",
    "for timestamp in Time:\n",
    "    rule_df = time_rules_map[timestamp].sort_values(by=['confidence'], ascending=False)\n",
    "    recommended_channels = get_channels_from_rules(rule_df[:considered_rules])\n",
    "    channel_names = \",\".join(appliance for appliance in recommended_channels)\n",
    "    if(channel_names == ''):\n",
    "        channel_names = 'None'\n",
    "    recommendation_list.append(channel_names)\n",
    "time_recommendation_df = pd.DataFrame({\"Time\" : Time, \"Recommendations\" : recommendation_list})\n",
    "print(\"Finished generating recommendations. Saving to file...\")\n",
    "\n",
    "time_recommendation_df.to_csv(recommendations_file)\n",
    "print(\"Recommendations saved to file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_recommendation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_channel_dict = get_ground_truth(test_dates, Time, channel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs_df = pd.read_csv(recommendations_file, header=0, names=['Time','Recommendations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_recs_dict = dict()\n",
    "for date in Dates:\n",
    "    for index,row in recs_df.iterrows():\n",
    "        time = row['Time']\n",
    "        recommendations = row['Recommendations']\n",
    "        datetime_recs_dict[date + \" \" + time] = set(list(recommendations.split(\",\")[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime_recs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3969298245614035\n"
     ]
    }
   ],
   "source": [
    "average_precision = calculate_precision(test_dates, Time, datetime_channel_dict, datetime_recs_dict)\n",
    "print(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "average_recall = calculate_recall(test_dates, Time, datetime_channel_dict, datetime_recs_dict)\n",
    "print(average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5159319963351318\n"
     ]
    }
   ],
   "source": [
    "average_f1_score = (2 * average_precision * average_recall)/(average_precision + average_recall)\n",
    "print(average_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdUAAAE/CAYAAAAQZlkTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARc0lEQVR4nO3df7Dld13f8de7u0ahgLRkdSQJbFoSdFXAdknp2FasNiagRlqnJGppgkyaaQLttFgyo7a2TNtQp45CgjsZjVRGjYONNspKqtSK/EjNBgIhweBOCGQJHTamQ4dISTe++8c92zlzuZt7Nrw3e+/m8Zi5M+f7/X7uOZ+bfPc8z/d7z/2e6u4AAF++P3eyJwAApwpRBYAhogoAQ0QVAIaIKgAMEVUAGLLzZD3w6aef3rt37z5ZDw8Aj8vtt9/+YHfv2mjbplGtqhuSfHeSz3b3N22wvZL8TJKXJfnTJJd29wc3u9/du3fnwIEDmw0DgC2lqj55rG2rnP59W5ILHmP7hUnOWXxdnuRnj2dyAHCq2DSq3f2eJA89xpCLkvxir7k1yTOr6uumJggA28XEG5XOSHL/0vKhxToAeFKZiGptsG7DCwpX1eVVdaCqDhw+fHjgoQFg65iI6qEkZy0tn5nkgY0Gdvf13b23u/fu2rXhG6cAYNuaiOrNSV5Va16S5HPd/ZmB+wWAbWWVP6n5lSQvTXJ6VR1K8q+SfEWSdPe+JPuz9uc0B7P2JzWXnajJAsBWtmlUu/uSTbZ3kivHZgQA25TLFALAEFEFgCGiCgBDRBUAhpy0T6kBkt1Xv/NkT+G43HfNy0/2FGBLc6QKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADFkpqlV1QVXdU1UHq+rqDbZ/dVX9ZlV9uKruqqrL5qcKAFvbplGtqh1JrktyYZI9SS6pqj3rhl2Z5O7ufmGSlyb5j1V12vBcAWBLW+VI9bwkB7v73u5+JMmNSS5aN6aTPL2qKsnTkjyU5MjoTAFgi1slqmckuX9p+dBi3bJrk3xDkgeS3Jnkn3T3n43MEAC2iVWiWhus63XL35XkjiTPTvKiJNdW1TO+5I6qLq+qA1V14PDhw8c9WQDYylaJ6qEkZy0tn5m1I9JllyW5qdccTPKJJF+//o66+/ru3tvde3ft2vV45wwAW9IqUb0tyTlVdfbizUcXJ7l53ZhPJfmOJKmqr03y/CT3Tk4UALa6nZsN6O4jVXVVkluS7EhyQ3ffVVVXLLbvS/LGJG+rqjuzdrr4Dd394AmcNwBsOZtGNUm6e3+S/evW7Vu6/UCS82enBgDbiysqAcAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYIioAsAQUQWAIaIKAENEFQCGiCoADBFVABgiqgAwRFQBYMhKUa2qC6rqnqo6WFVXH2PMS6vqjqq6q6p+f3aaALD17dxsQFXtSHJdkr+T5FCS26rq5u6+e2nMM5O8NckF3f2pqvqaEzVhANiqVjlSPS/Jwe6+t7sfSXJjkovWjfmBJDd196eSpLs/OztNANj6VonqGUnuX1o+tFi37Nwkf6Gq/ntV3V5Vr9rojqrq8qo6UFUHDh8+/PhmDABb1CpRrQ3W9brlnUn+apKXJ/muJD9eVed+yTd1X9/de7t7765du457sgCwlW36O9WsHZmetbR8ZpIHNhjzYHc/nOThqnpPkhcm+fjILAFgG1jlSPW2JOdU1dlVdVqSi5PcvG7Mf0nyN6tqZ1U9NclfS/Kx2akCwNa26ZFqdx+pqquS3JJkR5IbuvuuqrpisX1fd3+sqt6V5CNJ/izJz3X3R0/kxAFgq1nl9G+6e3+S/evW7Vu3/JNJfnJuagCwvbiiEgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGLLzZE8AgO1n99XvPNlTOC73XfPyJ+RxHKkCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIasFNWquqCq7qmqg1V19WOMe3FVPVpV3z83RQDYHjaNalXtSHJdkguT7ElySVXtOca4NyW5ZXqSALAdrHKkel6Sg919b3c/kuTGJBdtMO61Sf5zks8Ozg8Ato1VonpGkvuXlg8t1v1/VXVGklck2Tc3NQDYXlaJam2wrtct/3SSN3T3o495R1WXV9WBqjpw+PDhVecIANvCzhXGHEpy1tLymUkeWDdmb5IbqypJTk/ysqo60t2/sTyou69Pcn2S7N27d32YAWBbWyWqtyU5p6rOTvLpJBcn+YHlAd199tHbVfW2JL+1PqgAcKrbNKrdfaSqrsrau3p3JLmhu++qqisW2/0eFQCy2pFqunt/kv3r1m0Y0+6+9MufFgBsP66oBABDRBUAhogqAAwRVQAYIqoAMERUAWCIqALAEFEFgCGiCgBDRBUAhogqAAwRVQAYIqoAMERUAWCIqALAEFEFgCGiCgBDRBUAhogqAAwRVQAYIqoAMERUAWCIqALAkJ0newJTdl/9zpM9hZXdd83LT/YUADgBHKkCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgyEpRraoLquqeqjpYVVdvsP0Hq+oji6/3V9UL56cKAFvbplGtqh1JrktyYZI9SS6pqj3rhn0iybd19wuSvDHJ9dMTBYCtbpUj1fOSHOzue7v7kSQ3JrloeUB3v7+7/9di8dYkZ85OEwC2vlWiekaS+5eWDy3WHcsPJ/ntjTZU1eVVdaCqDhw+fHj1WQLANrBKVGuDdb3hwKpvz1pU37DR9u6+vrv3dvfeXbt2rT5LANgGdq4w5lCSs5aWz0zywPpBVfWCJD+X5MLu/pOZ6QHA9rHKkeptSc6pqrOr6rQkFye5eXlAVT0nyU1J/kF3f3x+mgCw9W16pNrdR6rqqiS3JNmR5Ibuvquqrlhs35fkXyZ5VpK3VlWSHOnuvSdu2gCw9axy+jfdvT/J/nXr9i3dfk2S18xODQC2F1dUAoAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwBBRBYAhogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgyEpRraoLquqeqjpYVVdvsL2q6s2L7R+pqr8yP1UA2No2jWpV7UhyXZILk+xJcklV7Vk37MIk5yy+Lk/ys8PzBIAtb5Uj1fOSHOzue7v7kSQ3Jrlo3ZiLkvxir7k1yTOr6uuG5woAW9oqUT0jyf1Ly4cW6453DACc0nauMKY2WNePY0yq6vKsnR5Oks9X1T0rPP7JdnqSByfvsN40eW+wofH9NrHv8oTYDvvuc4+1YZWoHkpy1tLymUkeeBxj0t3XJ7l+hcfcMqrqQHfvPdnzgONhv2W72u777iqnf29Lck5VnV1VpyW5OMnN68bcnORVi3cBvyTJ57r7M8NzBYAtbdMj1e4+UlVXJbklyY4kN3T3XVV1xWL7viT7k7wsycEkf5rkshM3ZQDYmqr7S371yZKqunxx2hq2Dfst29V233dFFQCGuEwhAAw5ZaJaVY9W1R1V9dGqekdVPXXgPvdW1ZsfY/uzq+rXvtzHgWNZt1//ZlU9c/j+L62qaxe3f6KqXj95/5zalvbPo1+7q+pZVfV7VfX5o/vWk8kpE9UkX+juF3X3NyV5JMkVyxsX70w+rp+3uw909+seY/sD3f39j2+6sJLl/fqhJFee7AnBkqP759Gv+5L8nyQ/nuQJfYFWVav8iegJdypFddkfJHne4lXTx6rqrUk+mOSsqjq/qj5QVR9cHNE+LUmq6sVV9f6q+nBV/WFVPb2qXlpVv7XY/m1Lr8Y+tNi+u6o+utj+VVX1C1V152L7ty/WX1pVN1XVu6rqj6vqP5yk/yZsfx/I0pXKqupHquq2xYdY/Oul9a9arPtwVb19se57qup/LPbN362qrz0J8+dJoLsf7u73Zi2ux1RV37h4rr1jsb+es1i/0f773Kp692L9u6vqOYv1b6uqn6qq30vypqr681V1w+LfxYeqav0ldU+4LVH2SYtXKxcmeddi1fOTXNbd/7iqTk/yY0m+s7sfrqo3JPlnVXVNkl9N8sruvq2qnpHkC+vu+vVJruzu9y1CvH6HuTJJuvubq+rrk/zXqjp3se1FSb4lyReT3FNVb+nu+wMrqrUPtviOJD+/WD4/ax9gcV7Wrmh2c1X9rSR/kuRHk3xrdz9YVX9xcRfvTfKS7u6qek2Sf5Hknz/BPwannqdU1R2L25/o7lccx/dekeRnuvuXFtdA2FFV35iN999rs3Z9+f9UVa9O8uYk37fYdm7WntMfrap/l+S/dferF78q+cOq+t3ufvjL/UFXdSpFdfl/7h9k7cnn2Uk+ubjIf5K8JGuftPO+qkqS07L26v/5ST7T3bclSXf/7yRZjDnqfUl+qqp+KclN3X1o3fa/keQti+//o6r6ZNb+ZyfJu7v7c4v7vDtrl7gSVVZxdL/eneT2JL+zWH/+4utDi+WnZS2yL0zya939YJJ090OL7Wcm+dVa+6CL05J84gmZPae6L3T3ix7n934gyY9W1ZlZe07946r629l4//3rSf7u4vbbkyyf8XtHdz+6uH1+ku9dem/AVyV5TpKPPc45HrdT6fTv8rn91y4+USdJll+hVJLfWRq3p7t/eLH+Mf+2qLuvSfKaJE9JcuviaHTZRtc/PuqLS7cfzan1YoYT6+iT1nOzFsOjv1OtJP9+aV9+Xnf/fI69L78lybXd/c1J/lHWnmzgCVNVr1j6Fdre7v7lJN+btbOCtyyCuulz8cLymPXP8X9v6d/Fc7r7CQtqcmpFdRW3JvnWqnpeklTVUxenaP8oybOr6sWL9U9f/0vvqvrL3X1nd78pyYEk66P6niQ/uBh7btZeHW2HDwxgG1ic6XhdktdX1Vdk7Qpnr156T8AZVfU1Sd6d5O9X1bMW64+ePvvqJJ9e3P6HT+jkIUl3//pS7A5U1V9Kcm93vzlrl7p9QY69/74/a5fITdaeZ997jIe5Jclra3Easaq+5QT9OMf0pDpi6u7DVXVpkl+pqq9crP6x7v54Vb0yyVuq6ilZe+X0neu+/Z8u3nz0aJK7k/x2kuXPjH1rkn1VdWeSI0ku7e4vrjtFDI9bd3+oqj6c5OLufntVfUOSDyz2sc8n+aHFJUT/bZLfr6pHs3Z6+NIkP5HkHVX16ay9uDz7ZPwMPDlU1X1JnpHktKr6viTnd/fd64a9MskPVdX/TfI/k/yb7n7oGPvv65LcUFU/kuRwjn0p3Dcm+ekkH1mE9b4k3z35s23GFZUAYMiT7fQvAJwwogoAQ0QVAIaIKgAMEVUAGCKqADBEVAFgiKgCwJD/BzeUJ8qX7l/nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Precision', 'Recall', 'F1-score']\n",
    "students = [0.30, 0.97, 0.46]\n",
    "ax.bar(langs,students, width = 0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPa8mxrUaeiUkqeP8Zwhgsv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "APRIORI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
